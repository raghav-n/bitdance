project:
    seed: 42
    device: "auto" # "cpu" | "cuda:0" | "mps"
    data_dir: "data"
    models_dir: "models"
    runs_dir: "runs"

dataset:
    slug: "restaurant_reviews"

augment:
    slug: "none" # logical augmentation scheme id
    classes:
        NoAdvertisement: 100
        NoIrrelevantContent: 100
        NoRantWithoutVisit: 100
    languages: ["en"]
    llm:
        gemini_model: "gemini-2.5-flash-lite"
        temperature: 1.2
        rate_limit: 0.2
    verify_conf_threshold: 0.8

ingest:
    sources:
        - name: "restaurant_reviews"
          path: "data/reviews.csv"
    image:
        resize_dimensions: [224, 224]
        format: "RGB"
        base_path: "data/dataset" # Updated path

annotate:
    llm:
        rate_limit: 0.2
        final_conf_threshold: 0.8
        startAt: 0

train:
    # family: "encoder" # "baseline" | "encoder" | "sft"
    # model_name: "xlm-roberta-base"
    # batch_size: 16
    # lr: 3e-5
    # epochs: 10
    multilabel: true
    train_jsonl: "data/processed/restaurant_reviews/train.jsonl"
    val_jsonl: "data/processed/restaurant_reviews/val.jsonl"
    use_modal: true
    modal_volume_name: "bitdance-models"  # All models share this Modal Volume
    modal_gpu: "A10"  # e.g., "T4", "L40S", "H100", "A100-80GB", or "H100:2"
    modal_download_artifacts: false  # if true, download remote artifacts after training
    modal_pull_only: false  # if true, skip training and only pull from Modal volume
    modal_timeout_seconds: 86400  # extend Modal function timeout for long trainings
    oversample: true
    # Optional: override remote relative subdir in Modal Volume to pull from
    # Example: models/encoder/20250830-113945
    # modal_pull_rel_subdir: "models/encoder/20250830-113945"

    # Optional: train multiple models in one run.
    # When provided, each item can override knobs; unspecified fields inherit from above.
    # If use_modal is true, each model runs in a separate container with the same Modal volume.
    models:
      - family: "encoder" # "baseline" | "encoder" | "sft"
        model_name: "distilbert-base-uncased"
        batch_size: 16
        lr: 3e-5
        epochs: 5
        run_name: "enc-distilbert-base"
        use_modal: false  # Train locally for faster setup

    #   - family: "encoder" # "baseline" | "encoder" | "sft"
    #     model_name: "bert-large-cased"
    #     batch_size: 16
    #     lr: 3e-5
    #     epochs: 10
    #     run_name: "enc-bert-large-cased"

    #   - family: "sft"
    #     model_name: "Qwen/Qwen3-8B"
    #     lr: 5e-5
    #     epochs: 3
    #     run_name: "sft-qwen-3-8b"

    #   - family: "sft"
    #     model_name: "mistralai/Ministral-8B-Instruct-2410"
    #     lr: 5e-5
    #     epochs: 3
    #     run_name: "sft-ministral-8b"

    #   - family: "sft"
    #     model_name: "google/gemma-3-12b-it"
    #     lr: 5e-5
    #     epochs: 3
    #     run_name: "sft-gemma-3-12b"

    #   - family: "fraud_scan"
    #     min_reviews: 5
    #     topk: 5
    #     overall_threshold: 0.6
    #     sim_threshold: 0.80
    #     high_sim_threshold: 0.90
    #     min_cluster_size: 2
    #     dup_threshold: 0.90

evaluate:
    threshold: 0.5
    use_modal: true
    modal_gpu: "A10"
    # modal_volume_name: "bitdance-models" # defaults to train.modal_volume_name
    modal_timeout_seconds: 7200
    # Batch generation during evaluation (per family defaults: sft=8, rnn=128, encoder=32)
    # batch_size: 8
    # sft_batch_size: 8
    # rnn_batch_size: 128
    # encoder_batch_size: 32
    # Log progress every N samples
    # log_every: 20
    # Evaluate multiple models stored remotely in the Modal volume.
    # Paths are relative to `models_dir` and match training outputs.
    models:
    #   - "sft/sft-qwen"
    #   - "sft/sft-mistral"
    #   - "sft/sft-gemma-3-4b"
    #   - "encoder/enc-xlm-roberta-base"
    #   - "encoder/enc-bert-large-cased"
      - "sft/sft-gemma-3-12b"
